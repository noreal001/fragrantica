#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Render
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
"""

import time
import json
from datetime import datetime
import logging
from typing import List, Dict, Optional
import re
import random
import requests
from bs4 import BeautifulSoup

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('simple_parser.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SimpleFragranticaParser:
    """–ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä —Å requests"""
    
    def __init__(self):
        self.base_url = "https://www.fragrantica.com"
        self.session = requests.Session()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        })
        
        logger.info("–ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _smart_delay(self, base_delay: float = 2.0):
        """–£–º–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —Å –≤–∞—Ä–∏–∞—Ü–∏–µ–π"""
        delay = base_delay + random.uniform(1.0, 3.0)
        time.sleep(delay)
    
    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
            
            # –£–º–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            self._smart_delay(3.0)
            
            # –ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ Cloudflare
            if 'cloudflare' in response.text.lower():
                logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ Cloudflare –∑–∞—â–∏—Ç–∞")
                return None
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            return soup
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            return None
    
    def extract_news_from_main_page(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        logger.info("–ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã Fragrantica...")
        
        soup = self.get_page_content(self.base_url)
        if not soup:
            return []
        
        news_items = []
        
        # –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤
        news_boxes = soup.find_all('div', class_='fr-news-box')
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_boxes)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤")
        
        for i, news_box in enumerate(news_boxes[:5], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 5 –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏
            try:
                news_item = self._extract_news_data(news_box)
                if news_item and news_item.get('link'):
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
                    full_content = self._get_article_content_safely(news_item['link'])
                    if full_content:
                        news_item['full_content'] = full_content
                    
                    news_items.append(news_item)
                    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å {i}: {news_item.get('title', '')[:50]}...")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –±–ª–æ–∫–∞ {i}: {e}")
                continue
        
        return news_items
    
    def _extract_news_data(self, news_box) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –±–ª–æ–∫–∞"""
        try:
            # –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_elem = news_box.find('h3')
            title = title_elem.get_text(strip=True) if title_elem else ""
            
            # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏
            link_elem = news_box.find('a')
            link = ""
            if link_elem and link_elem.get('href'):
                href = link_elem.get('href')
                link = self.base_url + href if href.startswith('/') else href
            
            # –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            img_elem = news_box.find('img')
            image_url = ""
            if img_elem and img_elem.get('src'):
                src = img_elem.get('src')
                image_url = self.base_url + src if src.startswith('/') else src
            
            # –ü–æ–∏—Å–∫ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è
            content_elem = news_box.find(['p', 'div'], class_=re.compile(r'content|description|excerpt', re.I))
            if not content_elem:
                content_elem = news_box.find('p')
            content = content_elem.get_text(strip=True) if content_elem else ""
            
            # –ü–æ–∏—Å–∫ –¥–∞—Ç—ã
            date_elem = news_box.find(['time', 'span', 'div'], class_=re.compile(r'date|time', re.I))
            date = date_elem.get_text(strip=True) if date_elem else ""
            
            if title:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫
                return {
                    'title': title,
                    'link': link,
                    'image_url': image_url,
                    'content': content,
                    'date': date,
                    'original_language': 'en'
                }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–∏: {e}")
        
        return None
    
    def _get_article_content_safely(self, article_url: str) -> Optional[str]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å—Ç–∞—Ç—å–∏"""
        try:
            logger.info(f"–ü—ã—Ç–∞—é—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏: {article_url}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å—Ç–∞—Ç—å–∏
            soup = self.get_page_content(article_url)
            if not soup:
                return None
            
            return self._extract_article_content(soup)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
            return None
    
    def _extract_article_content(self, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∏–∑ HTML —Å—Ç–∞—Ç—å–∏"""
        try:
            # –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏
            content_selectors = [
                'div.article-content',
                'div.post-content',
                'div.entry-content',
                'div.content',
                'article .content',
                '.main-content',
                '.article-body',
                '.post-body',
                '.news-content',
                '.article-text',
                '.post-text',
                '.entry-content',
                '.content-area',
                '.article',
                '.post',
                '.entry'
            ]
            
            full_content = ""
            
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                    for script in content_elem(["script", "style", "nav", "header", "footer", "aside"]):
                        script.decompose()
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
                    text = content_elem.get_text(separator='\n', strip=True)
                    if text and len(text) > 200:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        full_content = text
                        logger.info(f"–ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {selector}")
                        break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –∏—â–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            if not full_content:
                paragraphs = soup.find_all('p')
                if paragraphs:
                    texts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True) and len(p.get_text(strip=True)) > 50]
                    full_content = '\n\n'.join(texts)
                    logger.info(f"–ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
            
            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç, –∏—â–µ–º –ª—é–±–æ–π —Ç–µ–∫—Å—Ç –≤ —Å—Ç–∞—Ç—å–µ
            if not full_content:
                article_elem = soup.find('article') or soup.find('div', class_=re.compile(r'article|post|content', re.I))
                if article_elem:
                    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                    for script in article_elem(["script", "style", "nav", "header", "footer", "aside"]):
                        script.decompose()
                    
                    text = article_elem.get_text(separator='\n', strip=True)
                    if text and len(text) > 200:
                        full_content = text
                        logger.info("–ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏")
            
            # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            if full_content:
                # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
                full_content = re.sub(r'\n\s*\n', '\n\n', full_content)
                full_content = re.sub(r'\s+', ' ', full_content)
                full_content = full_content.strip()
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
                if len(full_content) > 4000:
                    full_content = full_content[:4000] + "..."
                
                return full_content
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
        
        return None
    
    def save_to_json(self, news_items: List[Dict], filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ JSON —Ñ–∞–π–ª"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"fragrantica_simple_news_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(news_items, f, ensure_ascii=False, indent=2)
            logger.info(f"–ù–æ–≤–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {filename}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
    
    def print_news_summary(self, news_items: List[Dict]):
        """–í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        print(f"\n{'='*60}")
        print(f"–ù–ê–ô–î–ï–ù–û –ù–û–í–û–°–¢–ï–ô: {len(news_items)}")
        print(f"{'='*60}")
        
        for i, news in enumerate(news_items, 1):
            print(f"\n{i}. {news.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')}")
            if news.get('date'):
                print(f"   üìÖ {news.get('date')}")
            if news.get('link'):
                print(f"   üîó {news.get('link')}")
            if news.get('content'):
                content_preview = news.get('content', '')[:100] + "..." if len(news.get('content', '')) > 100 else news.get('content', '')
                print(f"   üìù {content_preview}")
            if news.get('full_content'):
                full_content_preview = news.get('full_content', '')[:300] + "..." if len(news.get('full_content', '')) > 300 else news.get('full_content', '')
                print(f"   üìÑ –ü–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {full_content_preview}")
            print("-" * 60)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = None
    
    try:
        print("\n" + "="*60)
        print("üéØ –ü–†–û–°–¢–û–ô –ü–ê–†–°–ï–† FRAGRANTICA –î–õ–Ø RENDER")
        print("="*60)
        print("–≠—Ç–æ—Ç –ø–∞—Ä—Å–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏")
        print("="*60)
        
        parser = SimpleFragranticaParser()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–ª–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º
        logger.info("–ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞...")
        news_items = parser.extract_news_from_main_page()
        
        if not news_items:
            logger.warning("–ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –í–æ–∑–º–æ–∂–Ω–æ, –∏–∑–º–µ–Ω–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞.")
            print("\n‚ö†Ô∏è –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
            print("1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
            print("2. –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Å–∞–π—Ç Fragrantica –¥–æ—Å—Ç—É–ø–µ–Ω")
            return
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        parser.save_to_json(news_items)
        
        # –í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏
        parser.print_news_summary(news_items)
        
        logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        print("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤ —Ñ–∞–π–ª–µ simple_parser.log")

if __name__ == "__main__":
    main() 